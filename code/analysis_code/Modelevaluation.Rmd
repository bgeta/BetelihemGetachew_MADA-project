---
title: "5 fold logistic regression using tidymodels"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
library(tidyverse)
library(tidymodels)
library(pROC)

```

removing any missing data

```{r}
processeddata <- na.omit(processeddata)
processeddata <- as.data.frame(unclass(processeddata), stringsAsFactors = TRUE)
```

## Build models

**Train and Test Data Split and re-sampling folds**

-   create training and testing sets
-   create re-sampling folds from the *training* set

```{r}
set.seed(123)
processed_split <- initial_split(processeddata, strata=HSI)


HSI_train <- training(processed_split)
HSI_test <- testing(processed_split)

set.seed(234)
folds<-vfold_cv(data=HSI_train,v=5,Strata=HSI)
folds
```

```{r}
view(HSI_train)
```

**Setup recipe** #all predictors

```{r}
HSI_rec <- 
  recipe(HSI ~ A01+ RESIDENCE + AGE + A04 + A05 + A11 + Wealth + B04 + D08 + D01,data=HSI_train)%>% 
  step_dummy(all_nominal_predictors())
```

**Setup Model specifications**

```{r}
HSI_model <-
  logistic_reg() %>%
  set_engine("glm")%>%
set_mode("classification")

HSI_model
```

**Setup Workflow**

```{r}
HSI_wf <- workflow() %>%
  add_model(HSI_model) %>%
  add_recipe(HSI_rec)

HSI_wf
```

**Fit trained workflow model to training dataset to get the fitted model**

```{r}
fit1 <- 
HSI_wf %>% 
  fit(data = HSI_train)
fit1
```

```{r}
modelfit1<- fit1 %>%
  extract_fit_parsnip()%>%
  tidy()

modelfit1
```
*Estimating Model Performance*
We can now fit the above workflow to the re sampling folds to estimate performance of the model

```{r}
fold_fit <-
  HSI_wf %>%
  fit_resamples(folds)
fold_fit
```

```{r}
model1performance <-collect_metrics(fold_fit)
model1performance
```


The accuracy of .52 doesn't seem so great with this model.The roc_auc is also low, Ideally we would like the roc_auc to be closer to 1,


## ***Model II using 1 predictor only -  B04 (Age of smoking initiation as the main predictor)***

**Setup recipe**

```{r}
HSI_rec2 <- 
  recipe(HSI ~ B04,data=HSI_train)%>% 
  step_dummy(all_nominal_predictors())
```

**Setup Model specifications**

```{r}
HSI_model2 <-
  logistic_reg() %>%
  set_engine("glm")%>%
set_mode("classification")

HSI_model2
```

**Setup Workflow**

```{r}
HSI_wf2 <- workflow() %>%
  add_model(HSI_model) %>%
  add_recipe(HSI_rec2)

HSI_wf2
```

**Fit trained workflow model to training data set**

```{r}
fit2 <- 
HSI_wf2 %>% 
  fit(data = HSI_train)
```
looking at model output 
```{r}
modelfit2<-fit2 %>%
  extract_fit_parsnip()%>%
  tidy()
modelfit2
```
We can now fit the above workflow to the re sampling folds

```{r}
fold_fit2 <-
  HSI_wf2 %>%
  fit_resamples(folds)
fold_fit2
```

Take a look at the accuracy metrics

```{r}
model2performance<-collect_metrics(fold_fit2)
model2performance
```

The one predictor model seems to perform better with an accuracy of .56 and roc of .59

## ***Model III using D01 and D08 (quit attempt, quit interest as the main predictors)***

**Setup recipe**

```{r}
HSI_rec3 <- 
  recipe(HSI ~ D01,D08,data=HSI_train)%>% 
  step_dummy(all_nominal_predictors())
```

**Setup Model specifications**

```{r}
HSI_model3 <-
  logistic_reg() %>%
  set_engine("glm")%>%
set_mode("classification")

HSI_model3
```

**Setup Workflow**

```{r}
HSI_wf3 <- workflow() %>%
  add_model(HSI_model) %>%
  add_recipe(HSI_rec3)

HSI_wf3
```

**Fit trained workflow model to training data set**

```{r}
fit3 <- 
HSI_wf3 %>% 
  fit(data = HSI_train)
```
looking at model output 
```{r}
modelfit3 <- fit3 %>%
  extract_fit_parsnip()%>%
  tidy()

modelfit3
```


We can now fit the above workflow to the re sampling folds

```{r}
fold_fit3 <-
  HSI_wf3 %>%
  fit_resamples(folds)

fold_fit3
```

Take a look at the accuracy metrics

```{r}
model3performance<-collect_metrics(fold_fit3)
model3performance
```

This model seems to perform as good as model 1 but worse than model 2. We therefor choose model 2 to predict 

Now lets use the trained workflow fit 2 to predict test data set

```{r}
pred_prob <- predict(fit2,HSI_test,type = "prob")
```

Now lets look at the model and test data together using augment function

```{r}
HSI_aug2 <-
  augment(fit2,HSI_test,type="prob")
HSI_aug2
```

Plot the ROC curve to get the estimate

```{r}

ROC_Mod2 <- HSI_aug2%>% 
roc_curve(truth=HSI,.pred_LowAddiction) %>%
  autoplot()
ROC_Mod2
```
Getting the value of the Area under the curve would have been helpful but my calculations did not produce a value. But the curve here tells us the area under the curve is far away from 1. 


**Final Fit**
**Setup recipe**

```{r}
finalfit_rec <- 
  recipe(HSI ~ B04,data=HSI_test)%>% 
  step_dummy(all_nominal_predictors())
```

**Setup Model specifications**

```{r}
finalfit_mod <-
  logistic_reg() %>%
  set_engine("glm")%>%
set_mode("classification")

finalfit_mod
```

**Setup Workflow**

```{r}
finalfit_Wf <- workflow() %>%
  add_model(finalfit_mod) %>%
  add_recipe(finalfit_rec)

finalfit_Wf
```

**Fit trained workflow model to test data set**

```{r}
finalfit <- 
finalfit_Wf %>% 
  fit(data = HSI_test)
```
looking at model output 
```{r}
finalmodelfit<-finalfit %>%
  extract_fit_parsnip()%>%
  tidy()
finalmodelfit
```


```{r}
save_summary_location <- here::here("results","modelfit1.rds")
saveRDS(modelfit1, file = save_summary_location)
```

```{r}
save_summary_location <- here::here("results","modelfit2.rds")
saveRDS(modelfit2, file = save_summary_location)
```


```{r}
save_summary_location <- here::here("results","modelfit3.rds")
saveRDS(modelfit3, file = save_summary_location)
```


```{r}
save_summary_location <- here::here("results","model1performance.rds")
saveRDS(model1performance, file = save_summary_location)
```

```{r}
save_summary_location <- here::here("results","model2performance.rds")
saveRDS(model2performance, file = save_summary_location)
```

```{r}
save_summary_location <- here::here("results","model3performance.rds")
saveRDS(model3performance, file = save_summary_location)
```

```{r}
figure_file <- here::here("results","ROC_Mod2.png")
ggsave(filename = figure_file, plot=ROC_Mod2)
```